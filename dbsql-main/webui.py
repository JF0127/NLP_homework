import gradio as gr
import pandas as pd
import datetime
import time
import schedule # ç”¨äºå®šæ—¶ä»»åŠ¡
import requests # ç”¨äºç›´æ¥è°ƒç”¨Ollama HTTP API
import json     # ç”¨äºæ„é€ å’Œè§£æJSON
from collections import defaultdict # ç”¨äºèšåˆæ•°æ®
import mysql.connector
from mysql.connector import Error as MySQLError # ä¸ºæ¸…æ™°èµ·è§ï¼Œé‡å‘½åError

# --- 1. ä»ä½ çš„é¡¹ç›®ä¸­å¯¼å…¥DBSQLAnswer ---
try:
    from dbsql.llm.chains.dbsql_answer import DBSQLAnswer
    DBSQLAnswer_available = True
except ImportError:
    print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥ DBSQLAnswer ç±»ã€‚Text2SQLé¡µé¢å°†ä½¿ç”¨æ¨¡æ‹ŸåŠŸèƒ½ã€‚")
    print("è¯·ç¡®ä¿ dbsql.llm.chains.dbsql_answer è·¯å¾„æ­£ç¡®ä¸”åœ¨PYTHONPATHä¸­ã€‚")
    DBSQLAnswer_available = False
    class DBSQLAnswer: # æ¨¡æ‹Ÿç±»ï¼Œå¦‚æœå¯¼å…¥å¤±è´¥
        def __init__(self, model, db_type, db_host, db_port, db_user, db_password, db_name):
            self.db_name = db_name
            print(f"è­¦å‘Šï¼šä½¿ç”¨äº†æ¨¡æ‹Ÿçš„DBSQLAnswer (æ•°æ®åº“: {db_name})")
        def step_run(self, question: str):
            time.sleep(0.5) # æ¨¡æ‹Ÿå¤„ç†
            if "å¤šå°‘" in question:
                 return f"SELECT COUNT(*) FROM DUMMY_TABLE WHERE q='{question}'", f"æ¨¡æ‹Ÿç»“æœï¼šå…³äº '{question}' æœ‰100æ¡è®°å½•ã€‚"
            return f"-- æ¨¡æ‹ŸSQL for: {question} --", f"æ¨¡æ‹Ÿçš„è‡ªç„¶è¯­è¨€å›ç­”: {question}"

# --- 2. å…¨å±€é…ç½® ---
DB_CONNECTION_CONFIG = {
    'user': 'root',
    'password': 'jhl12735800', # !!! è¯·åŠ¡å¿…ä½¿ç”¨ä½ çš„çœŸå®å¯†ç  !!!
    'host': '127.0.0.1',
    'charset': 'utf8mb4',
}
MAIN_DB_NAME = 'NLP_DB_BY_TYPE'
TABLE_NAME_PREFIX = ''

EXPLICIT_COLUMN_SCHEMAS = {
    "id": {"type": "INT AUTO_INCREMENT PRIMARY KEY", "comment": 'è‡ªåŠ¨é€’å¢ä¸»é”®'},
    "time": {"type": "DATE", "comment": 'æŠ•è¯‰å‘ç”Ÿçš„æ—¥æœŸ, æ ¼å¼YYYY-MM-DD'},
    "amount": {"type": "DECIMAL(10,2)", "comment": 'æ¶‰åŠé‡‘é¢'},
    "subject": {"type": "VARCHAR(255)", "comment": 'æŠ•è¯‰æ–¹'},
    "object": {"type": "VARCHAR(255)", "comment": 'è¢«æŠ•è¯‰æ–¹'},
    "description": {"type": "TEXT", "comment": 'æŠ•è¯‰çš„å…·ä½“å†…å®¹æè¿°'},
    "platform": {"type": "VARCHAR(255)", "comment": 'æŠ•è¯‰å‘ç”Ÿçš„å¹³å°'},
    "complaint_type": {"type": "VARCHAR(255)", "comment": 'æŠ•è¯‰çš„å®é™…åˆ†ç±»å€¼'}
}
TIME_COLUMN_NAME = 'time'

# Ollamaæ¨¡å‹é…ç½® (ç”¨äºé¡µé¢ä¸€çš„è¶‹åŠ¿é¢„è­¦)
# è¿™äº›å€¼æ¥è‡ªä½ ä¹‹å‰æä¾›çš„ final_trend_analysis_script.py
OLLAMA_MODEL_FOR_TREND_ANALYSIS = "deepseek-r1:14b" 
OLLAMA_BASE_URL_FOR_TRENDS = "https://u354342-baf8-f3ff1b79.bjc1.seetacloud.com:8443" # ä½ è„šæœ¬ä¸­æä¾›çš„URL

# Text2SQL ä½¿ç”¨çš„ DBSQLAnswer å®ä¾‹
if DBSQLAnswer_available:
    try:
        db_answer_instance_for_text2sql = DBSQLAnswer(
            model="local", db_type="MySQL", db_host=DB_CONNECTION_CONFIG['host'],
            db_port=3306, db_user=DB_CONNECTION_CONFIG['user'],
            db_password=DB_CONNECTION_CONFIG['password'], db_name=MAIN_DB_NAME,
        )
        print("DBSQLAnswer å®ä¾‹ (ç”¨äºText2SQL) å·²æˆåŠŸåˆå§‹åŒ–ã€‚")
    except Exception as e:
        print(f"é”™è¯¯ï¼šåˆå§‹åŒ–çœŸå® DBSQLAnswer å¤±è´¥: {e}ã€‚Text2SQLå°†ä½¿ç”¨æ¨¡æ‹ŸåŠŸèƒ½ã€‚")
        db_answer_instance_for_text2sql = DBSQLAnswer("simulated", "MySQL", "localhost", 3306, "user", "pass", "db")
else:
    db_answer_instance_for_text2sql = DBSQLAnswer("simulated", "MySQL", "localhost", 3306, "user", "pass", "db")

SCHEDULED_ANALYSIS_TIME = "02:00"

# --- 3. æ•°æ®åº“è¾…åŠ©å‡½æ•° ---
def connect_db(database_name=None):
    conn = None; config = DB_CONNECTION_CONFIG.copy()
    if database_name: config['database'] = database_name
    try: conn = mysql.connector.connect(**config)
    except MySQLError as e: print(f"é”™è¯¯ï¼šè¿æ¥MySQLæ•°æ®åº“ '{database_name if database_name else 'æœåŠ¡å™¨'}' å¤±è´¥: {e}")
    return conn

def get_all_complaint_table_names(db_conn, prefix):
    if not db_conn or not db_conn.is_connected(): return []
    names = []; cursor = None
    try:
        cursor = db_conn.cursor()
        cursor.execute("SHOW TABLES;")
        for row in cursor.fetchall():
            if row[0].startswith(prefix): names.append(row[0])
    except MySQLError as e: print(f"é”™è¯¯ï¼šè·å–è¡¨ååˆ—è¡¨æ—¶å‡ºé”™: {e}")
    finally: 
        if cursor: cursor.close()
    return names

# --- 4. æ•°æ®è·å–å‡½æ•° (çœŸå®æ•°æ® for Page 1 & Alerts) ---
def fetch_complaint_volume_data_from_db():
    print(f"æ•°æ®åº“æ“ä½œï¼šæ­£åœ¨è·å–2023-2024å¹´åº¦æœˆåº¦æŠ•è¯‰æ€»é‡...")
    conn = connect_db(MAIN_DB_NAME)
    if not conn: return pd.DataFrame({'æœˆä»½': [], 'æ¯æœˆæŠ•è¯‰é‡': []})

    all_table_names = get_all_complaint_table_names(conn, TABLE_NAME_PREFIX)
    daily_counts_aggregated = defaultdict(int)

    start_dt = pd.to_datetime("2023-01-01", utc=False)
    end_dt = pd.to_datetime("2024-12-31", utc=False) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)

    if all_table_names:
        for table_name in all_table_names:
            cursor = None
            try:
                cursor = conn.cursor(dictionary=True)
                query = f"SELECT `{TIME_COLUMN_NAME}`, COUNT(*) as count FROM `{table_name}` GROUP BY `{TIME_COLUMN_NAME}`"
                cursor.execute(query)
                for row in cursor.fetchall():
                    event_date_obj = row[TIME_COLUMN_NAME]
                    if event_date_obj is not None:
                        event_date = pd.to_datetime(event_date_obj, utc=False)
                        if event_date < start_dt or event_date > end_dt:
                            continue
                        daily_counts_aggregated[event_date] += row['count']
            except MySQLError as e: print(f"é”™è¯¯ï¼šä»è¡¨ '{table_name}' æŸ¥è¯¢æ¯æ—¥æŠ•è¯‰é‡æ—¶å‡ºé”™: {e}")
            finally:
                if cursor: cursor.close()
    if conn.is_connected(): conn.close()

    if not daily_counts_aggregated: return pd.DataFrame({'æœˆä»½': [], 'æ¯æœˆæŠ•è¯‰é‡': []})

    df = pd.DataFrame(list(daily_counts_aggregated.items()), columns=['æ—¥æœŸ', 'æ¯æ—¥æŠ•è¯‰é‡'])
    if df.empty: return pd.DataFrame({'æœˆä»½': [], 'æ¯æœˆæŠ•è¯‰é‡': []})

    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    df = df.sort_values(by='æ—¥æœŸ')
    df.set_index('æ—¥æœŸ', inplace=True)

    monthly_df = df['æ¯æ—¥æŠ•è¯‰é‡'].resample('MS').sum().reset_index()
    monthly_df.rename(columns={'æ—¥æœŸ': 'æœˆä»½', 'æ¯æ—¥æŠ•è¯‰é‡': 'æ¯æœˆæŠ•è¯‰é‡'}, inplace=True)
    
    # --- ä¿®æ”¹ï¼šç¡®ä¿æœˆä»½åˆ—æ˜¯ 'YYYY-MM' æ ¼å¼çš„å­—ç¬¦ä¸² ---
    if not monthly_df.empty:
        monthly_df['æœˆä»½'] = pd.to_datetime(monthly_df['æœˆä»½']).dt.strftime('%Y-%m')
    # --- ä¿®æ”¹ç»“æŸ ---

    print(f"æ•°æ®åº“æ“ä½œï¼šæˆåŠŸè·å–å¹¶èšåˆäº†2023-2024å¹´åº¦ {len(monthly_df)} ä¸ªæœˆçš„æŠ•è¯‰æ€»é‡æ•°æ®ã€‚")
    return monthly_df

def fetch_complaint_types_trend_from_db():
    # ... (å‰é¢çš„æ•°æ®åº“è¿æ¥å’Œæ•°æ®è·å–é€»è¾‘ä¸å˜) ...
    print(f"æ•°æ®åº“æ“ä½œï¼šæ­£åœ¨è·å–2023-2024å¹´åº¦å„æŠ•è¯‰ç±»å‹æœˆåº¦è¶‹åŠ¿...")
    conn = connect_db(MAIN_DB_NAME)
    if not conn: return pd.DataFrame({'æœˆä»½': [], 'æŠ•è¯‰ç±»å‹': [], 'æ•°é‡': []})

    all_table_names = get_all_complaint_table_names(conn, TABLE_NAME_PREFIX)
    all_type_trends_daily = []

    start_dt = pd.to_datetime("2023-01-01", utc=False)
    end_dt = pd.to_datetime("2024-12-31", utc=False) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)

    if all_table_names:
        for table_name in all_table_names:
            cursor = None
            try:
                cursor = conn.cursor(dictionary=True)
                type_name = table_name.replace(TABLE_NAME_PREFIX, "")
                query = f"SELECT `{TIME_COLUMN_NAME}`, COUNT(*) as count FROM `{table_name}` GROUP BY `{TIME_COLUMN_NAME}`"
                cursor.execute(query)
                for row in cursor.fetchall():
                    event_date_obj = row[TIME_COLUMN_NAME]
                    if event_date_obj is not None:
                        event_date = pd.to_datetime(event_date_obj, utc=False)
                        if event_date < start_dt or event_date > end_dt:
                            continue
                        all_type_trends_daily.append({'æ—¥æœŸ': event_date, 'æŠ•è¯‰ç±»å‹': type_name, 'æ•°é‡': row['count']})
            except MySQLError as e: print(f"é”™è¯¯ï¼šä»è¡¨ '{table_name}' æŸ¥è¯¢ç±»å‹è¶‹åŠ¿æ—¶å‡ºé”™: {e}")
            finally:
                if cursor: cursor.close()
    if conn.is_connected(): conn.close()

    if not all_type_trends_daily: return pd.DataFrame({'æœˆä»½': [], 'æŠ•è¯‰ç±»å‹': [], 'æ•°é‡': []})

    df = pd.DataFrame(all_type_trends_daily)
    if df.empty: return pd.DataFrame({'æœˆä»½': [], 'æŠ•è¯‰ç±»å‹': [], 'æ•°é‡': []})

    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    df = df.sort_values(by=['æŠ•è¯‰ç±»å‹', 'æ—¥æœŸ'])

    monthly_dfs = []
    for type_name_group, group in df.groupby('æŠ•è¯‰ç±»å‹'):
        group = group.set_index('æ—¥æœŸ')
        monthly_group = group['æ•°é‡'].resample('MS').sum().reset_index()
        monthly_group['æŠ•è¯‰ç±»å‹'] = type_name_group
        monthly_dfs.append(monthly_group)

    if not monthly_dfs: return pd.DataFrame({'æœˆä»½': [], 'æŠ•è¯‰ç±»å‹': [], 'æ•°é‡': []})
    
    final_df = pd.concat(monthly_dfs)
    final_df.rename(columns={'æ—¥æœŸ': 'æœˆä»½'}, inplace=True)
    
    # --- ä¿®æ”¹ï¼šç¡®ä¿æœˆä»½åˆ—æ˜¯ 'YYYY-MM' æ ¼å¼çš„å­—ç¬¦ä¸² ---
    if not final_df.empty:
        final_df['æœˆä»½'] = pd.to_datetime(final_df['æœˆä»½']).dt.strftime('%Y-%m')
    # --- ä¿®æ”¹ç»“æŸ ---

    final_df = final_df[['æœˆä»½', 'æŠ•è¯‰ç±»å‹', 'æ•°é‡']]
    print(f"æ•°æ®åº“æ“ä½œï¼šæˆåŠŸè·å–å¹¶èšåˆäº†2023-2024å¹´åº¦ {len(final_df)} æ¡å„æŠ•è¯‰ç±»å‹çš„æœˆåº¦æ—¶é—´è¶‹åŠ¿æ•°æ®ã€‚")
    return final_df


def get_overall_n_newest_complaints(n_records=50):
    """ä»æ‰€æœ‰ç›¸å…³æŠ•è¯‰è¡¨ä¸­è·å–æŒ‰æ—¶é—´æ’åºæœ€æ–°çš„ N æ¡è¯¦ç»†è®°å½•ã€‚"""
    print(f"æ•°æ®åº“æ“ä½œï¼šæ­£åœ¨è·å–å…¨å±€æœ€æ–°çš„ {n_records} æ¡æŠ•è¯‰è®°å½•...")
    db_main_conn = connect_db(database_name=MAIN_DB_NAME)
    if not db_main_conn: return []
    all_complaints_ever_list = []
    complaint_table_names = get_all_complaint_table_names(db_main_conn, TABLE_NAME_PREFIX)
    if not complaint_table_names:
        print(f"ä¿¡æ¯ï¼šåœ¨æ•°æ®åº“ '{MAIN_DB_NAME}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»¥ '{TABLE_NAME_PREFIX}' å¼€å¤´çš„è¡¨ (è·å–æœ€æ–°Næ¡)ã€‚")
    else:
        columns_to_fetch = list(EXPLICIT_COLUMN_SCHEMAS.keys())
        for table_name in complaint_table_names:
            cursor = None
            try:
                cursor = db_main_conn.cursor(dictionary=True)
                select_cols_str = ", ".join([f"`{col}`" for col in columns_to_fetch])
                # ä¸ºäº†è·å–æœ€æ–°çš„Næ¡ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦å…ˆè·å–è¾ƒå¤šæ•°æ®æˆ–æ‰€æœ‰æ•°æ®è¿›è¡Œå…¨å±€æ’åº
                # å¦‚æœè¡¨éå¸¸å¤§ï¼Œè¿™é‡Œåº”è¯¥ä¼˜åŒ–ä¸ºæ¯ä¸ªè¡¨å–æœ€æ–°çš„Næ¡ï¼ˆæˆ–ç¨å¤šäºN/num_tablesæ¡ï¼‰ç„¶ååˆå¹¶æ’åº
                # å½“å‰å®ç°æ˜¯è·å–æ‰€æœ‰ï¼Œç„¶ååœ¨Pythonä¸­æ’åºï¼Œä¸ä½ ä¹‹å‰çš„è„šæœ¬ä¸€è‡´
                query = f"SELECT {select_cols_str} FROM `{table_name}`" 
                cursor.execute(query)
                records_from_this_table = cursor.fetchall()
                category_name = table_name.replace(TABLE_NAME_PREFIX, "")
                for record_dict in records_from_this_table:
                    if record_dict.get(TIME_COLUMN_NAME) is not None:
                        record_dict['source_table_category'] = category_name
                        all_complaints_ever_list.append(record_dict)
            except MySQLError as e: print(f"é”™è¯¯ï¼šä»è¡¨ '{table_name}' è·å–æ‰€æœ‰æ•°æ®æ—¶å‡ºé”™: {e}")
            finally: 
                if cursor: cursor.close()
    if db_main_conn.is_connected(): db_main_conn.close()
    if not all_complaints_ever_list:
        print("ä¿¡æ¯ï¼šæœªèƒ½ä»ä»»ä½•è¡¨ä¸­è·å–åˆ°ç”¨äºå…¨å±€æ’åºçš„æ•°æ®ã€‚")
        return []
    try:
        all_complaints_ever_list.sort(key=lambda x: x[TIME_COLUMN_NAME], reverse=True)
    except (TypeError, KeyError) as e: # KeyError if TIME_COLUMN_NAME is missing in some dict
        print(f"é”™è¯¯ï¼šæ’åºæ—¶å‘ç”Ÿé”™è¯¯ (å¯èƒ½æ˜¯å› ä¸º '{TIME_COLUMN_NAME}' å­—æ®µé—®é¢˜): {e}")
        return [] 
    top_n_complaints = all_complaints_ever_list[:n_records]
    print(f"ä¿¡æ¯ï¼šæ€»å…±ä»æ‰€æœ‰è¡¨ä¸­è·å–å¹¶æ’åºäº† {len(all_complaints_ever_list)} æ¡è®°å½•ï¼Œè¿”å›æœ€æ–°çš„ {len(top_n_complaints)} æ¡ã€‚")
    return top_n_complaints

def fetch_complaint_types_distribution_from_db():
    print("æ•°æ®åº“æ“ä½œï¼šæ­£åœ¨è·å–å„æŠ•è¯‰ç±»å‹åˆ†å¸ƒ...")
    conn = connect_db(MAIN_DB_NAME)
    if not conn: return pd.DataFrame({'æŠ•è¯‰ç±»å‹': [], 'æ•°é‡': []})
    all_table_names = get_all_complaint_table_names(conn, TABLE_NAME_PREFIX)
    type_counts = []
    if all_table_names:
        for table_name in all_table_names:
            cursor = None
            try:
                cursor = conn.cursor() # New cursor for each query
                query = f"SELECT COUNT(*) FROM `{table_name}`"
                cursor.execute(query)
                count_result = cursor.fetchone()
                count = count_result[0] if count_result else 0
                type_name = table_name.replace(TABLE_NAME_PREFIX, "")
                type_counts.append({'æŠ•è¯‰ç±»å‹': type_name, 'æ•°é‡': count})
            except MySQLError as e: print(f"é”™è¯¯ï¼šä»è¡¨ '{table_name}' æŸ¥è¯¢æ€»æ•°æ—¶å‡ºé”™: {e}")
            finally: 
                if cursor: cursor.close()
    if conn.is_connected(): conn.close()
    df = pd.DataFrame(type_counts)
    print(f"æ•°æ®åº“æ“ä½œï¼šæˆåŠŸè·å–äº† {len(df)} ä¸ªæŠ•è¯‰ç±»å‹çš„åˆ†å¸ƒæ•°æ®ã€‚")
    return df

# --- 5. Ollama LLM äº¤äº’å‡½æ•° (ä½¿ç”¨requests.post) ---
def analyze_complaint_trends_with_ollama_via_requests(
        complaints_data_list, ollama_model_name, base_url, analysis_type_description=""):
    if not complaints_data_list: return "åˆ†æä¸­æ­¢ï¼šè¾“å…¥æ•°æ®ä¸ºç©ºã€‚"
    if base_url == "YOUR_OLLAMA_BASE_URL_HERE" or not base_url or "seetacloud.com" not in base_url : # Added check for placeholder or unconfigured SeetaCloud URL
        print(f"é”™è¯¯ï¼šOllamaåŸºç¡€URL ({base_url}) æœªæ­£ç¡®é…ç½®æˆ–ä»ä¸ºå ä½ç¬¦ã€‚")
        return f"é…ç½®é”™è¯¯ï¼šOllamaåŸºç¡€URL ({base_url}) æœªæ­£ç¡®è®¾ç½®ã€‚"
    # (The rest of this function with the detailed prompt is the same as provided in the previous response)
    # For brevity, I'll just include the key call part. Ensure you have the full prompt logic here.
    print(f"\n--- Ollama(requests)è¶‹åŠ¿åˆ†æï¼šæ¨¡å‹ '{ollama_model_name}' URL '{base_url}' åˆ†æ {analysis_type_description} ---")
    system_prompt_content = (
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸ºæ”¿åºœéƒ¨é—¨æä¾›å†³ç­–æ”¯æŒçš„èµ„æ·±å…¬å…±æ”¿ç­–ä¸æ¶ˆè´¹è€…è¡Œä¸ºåˆ†æé¡¾é—®ã€‚ä½ çš„æ ¸å¿ƒä¸“é•¿æ˜¯ä»å¤§è§„æ¨¡ã€é«˜æ—¶æ•ˆæ€§çš„æ¶ˆè´¹è€…æŠ•è¯‰æ•°æ®ï¼ˆä¾‹å¦‚12315ç³»ç»Ÿæ•°æ®ï¼‰ä¸­æ•é”æ´å¯Ÿç¤¾ä¼šç»æµè¿è¡Œä¸­çš„å…³é”®ä¿¡å·ï¼Œ"
        "è¯†åˆ«äºŸå¾…å…³æ³¨çš„æ¶ˆè´¹é¢†åŸŸçƒ­ç‚¹é—®é¢˜ï¼Œå¹¶ç²¾å‡†ç ”åˆ¤å…¶åŠ¨æ€æ¼”åŒ–è¶‹åŠ¿ã€‚\n"
        "ä½ çš„åˆ†ææŠ¥å‘Šå°†ä½œä¸ºæ”¿åºœç›¸å…³éƒ¨é—¨è¿›è¡Œå¸‚åœºç›‘ç®¡ã€æ”¿ç­–åˆ¶å®šå’Œé£é™©é¢„è­¦çš„é‡è¦å‚è€ƒã€‚\n\n"
        "åŸºäºæ¥ä¸‹æ¥æä¾›çš„æ¶ˆè´¹è€…æŠ•è¯‰æ•°æ®æ‘˜è¦ï¼Œè¯·åŠ¡å¿…å®Œæˆä»¥ä¸‹æ ¸å¿ƒåˆ†æä»»åŠ¡ï¼š\n"
        "1.  **å½“å‰æ¶ˆè´¹çƒ­ç‚¹é—®é¢˜è¯†åˆ«**ï¼š\n"
        "    - æ˜ç¡®æŒ‡å‡ºå½“å‰æ•°æ®ä¸­åæ˜ å‡ºçš„ã€åœ¨ç¤¾ä¼šç»æµç”Ÿæ´»ä¸­æœ€ä¸ºçªå‡ºæˆ–äºŸå¾…å…³æ³¨çš„æ¶ˆè´¹çƒ­ç‚¹é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šç‰¹å®šå•†å“/æœåŠ¡æŠ•è¯‰æ¿€å¢ã€æ–°å‹æ¶ˆè´¹é™·é˜±ã€æ¶‰åŠé¢†åŸŸå¹¿æ³›çš„å…±æ€§é—®é¢˜ç­‰ï¼‰ã€‚\n"
        "    - å¯¹æ¯ä¸ªçƒ­ç‚¹é—®é¢˜è¿›è¡Œç®€è¦æè¿°ï¼Œè¯´æ˜å…¶ä¸»è¦è¡¨ç°å½¢å¼å’Œæ¶‰åŠçš„æ¶ˆè´¹é¢†åŸŸã€‚\n"
        "2.  **æŠ•è¯‰åŠ¨æ€è¶‹åŠ¿åˆ†æ**ï¼š\n"
        "    - ç»“åˆæ‰€æä¾›æ•°æ®çš„æ—¶é—´ä¿¡æ¯ï¼Œåˆ†æå„ç±»ä¸»è¦æŠ•è¯‰ï¼ˆæˆ–ä½ è¯†åˆ«å‡ºçš„çƒ­ç‚¹é—®é¢˜ï¼‰éšæ—¶é—´æ¼”å˜çš„åŠ¨æ€ç‰¹å¾ï¼ˆä¾‹å¦‚ï¼šæ˜¯å¿«é€Ÿå¢é•¿ã€æŒç»­å¹³ç¨³ã€å­£èŠ‚æ€§æ³¢åŠ¨ï¼Œè¿˜æ˜¯å¶å‘æ€§çˆ†å‘ï¼Ÿï¼‰ã€‚\n"
        "    - å¦‚æœæ•°æ®æ”¯æŒï¼Œå°è¯•æŒ‡å‡ºè¶‹åŠ¿èƒŒåçš„å¯èƒ½é©±åŠ¨å› ç´ ã€‚\n"
        "3.  **æ½œåœ¨å½±å“ä¸é£é™©ç ”åˆ¤**ï¼š\n"
        "    - è¯„ä¼°å·²è¯†åˆ«çš„æ¶ˆè´¹çƒ­ç‚¹å’Œè´Ÿé¢è¶‹åŠ¿å¯èƒ½å¯¹æ¶ˆè´¹è€…åˆæ³•æƒç›Šã€å¸‚åœºç»æµç§©åºä»¥åŠç¤¾ä¼šå’Œè°ç¨³å®šé€ æˆçš„æ½œåœ¨å½±å“å’Œé£é™©çº§åˆ«ã€‚\n"
        "4.  **æ”¿ç­–å…³æ³¨ä¸å»ºè®®æ–¹å‘**ï¼ˆæ­¤éƒ¨åˆ†éœ€å®¡æ…ï¼ŒåŸºäºæ•°æ®å®¢è§‚æå‡ºï¼‰ï¼š\n"
        "    - æ ¹æ®åˆ†æç»“æœï¼Œå‡ç»ƒå‡ºéœ€è¦æ”¿åºœç›‘ç®¡éƒ¨é—¨æˆ–ç›¸å…³æ”¿ç­–åˆ¶å®šè€…é‡ç‚¹å…³æ³¨çš„é¢†åŸŸæˆ–å…·ä½“é—®é¢˜ç‚¹ã€‚\n"
        "    - ï¼ˆå¦‚æœæ•°æ®å’Œä½ çš„åˆ†æèƒ½æ”¯æŒï¼‰å¯ä»¥åˆæ­¥æå‡ºéœ€è¦è¿›ä¸€æ­¥è°ƒç ”æˆ–è€ƒè™‘çš„æ”¿ç­–è°ƒæ•´æ–¹å‘ã€‚\n\n"
        "åˆ†ææŠ¥å‘Šçš„æ€»ä½“è¦æ±‚ï¼š\n"
        "-   **é«˜åº¦å®¢è§‚**ï¼šä¸¥æ ¼åŸºäºæ•°æ®è¿›è¡Œåˆ†æï¼Œé¿å…æ— ä¾æ®çš„æ¨æµ‹ã€‚\n"
        "-   **é‡ç‚¹çªå‡º**ï¼šä¼˜å…ˆå‘ˆç°å¯¹æ”¿åºœå†³ç­–æœ€å…·ä»·å€¼çš„æ ¸å¿ƒå‘ç°ã€‚\n"
        "-   **ç»“æ„åŒ–å‘ˆç°**ï¼šæŠ¥å‘Šä¸»ä½“å¯è€ƒè™‘é‡‡ç”¨å¦‚â€œä¸€ã€å½“å‰ä¸»è¦æ¶ˆè´¹çƒ­ç‚¹é—®é¢˜â€ã€â€œäºŒã€æŠ•è¯‰åŠ¨æ€è¶‹åŠ¿è§‚å¯Ÿâ€ã€â€œä¸‰ã€æ½œåœ¨å½±å“ä¸é£é™©è¯„ä¼°â€ã€â€œå››ã€æ”¿ç­–å…³æ³¨å»ºè®®â€ç­‰é€»è¾‘æ¸…æ™°çš„ç« èŠ‚è¿›è¡Œç»„ç»‡ã€‚\n"
        "-   **è¯­è¨€ä¸“ä¸š**ï¼šä½¿ç”¨ä¸“ä¸šã€ä¸¥è°¨ã€ç²¾ç‚¼çš„è¯­è¨€ï¼Œé¿å…å£è¯­åŒ–å’Œæ¨¡ç³Šè¡¨è¾¾ï¼Œç¡®ä¿æŠ¥å‘Šçš„æƒå¨æ€§ã€‚"
    )
    user_prompt_lines = [
        f"èƒŒæ™¯ä¿¡æ¯ï¼šä»¥ä¸‹æ•°æ®æŠ½æ ·è‡ª12315æ¶ˆè´¹è€…æŠ•è¯‰ä¸¾æŠ¥ç³»ç»Ÿï¼Œåæ˜ äº†åœ¨â€œ{analysis_type_description}â€æ—¶é—´çª—å£å†…çš„éƒ¨åˆ†æ¶ˆè´¹è€…æ ¸å¿ƒè¯‰æ±‚ã€‚",
        "æ ¸å¿ƒåˆ†æä»»åŠ¡ï¼šè¯·æ‚¨ä½œä¸ºèµ„æ·±æ”¿åºœåˆ†æé¡¾é—®ï¼Œä¸¥æ ¼éµç…§ç³»ç»Ÿæç¤ºä¸­å®šä¹‰çš„è§’è‰²èŒè´£ã€åˆ†ææ¡†æ¶å’ŒæŠ¥å‘Šè¦æ±‚ï¼Œå¯¹ä¸‹è¿°æ•°æ®è¿›è¡Œæ·±å…¥åˆ†æï¼Œæ—¨åœ¨ï¼š",
        "  (A) æœ‰æ•ˆè¯†åˆ«å½“å‰ç¤¾ä¼šç»æµç”Ÿæ´»ä¸­äºŸå¾…å…³æ³¨çš„æ¶ˆè´¹çƒ­ç‚¹é—®é¢˜ï¼›",
        "  (B) ç²¾å‡†åˆ†æå„ç±»æŠ•è¯‰éšæ—¶é—´æ¼”å˜çš„åŠ¨æ€è¶‹åŠ¿åŠå…¶æ½œåœ¨å½±å“ã€‚",
        "è¯·ç¡®ä¿æ‚¨çš„åˆ†æå…·æœ‰å‰ç»æ€§å’Œå†³ç­–å‚è€ƒä»·å€¼ã€‚\n",
        "å…·ä½“çš„æŠ•è¯‰æ•°æ®æ¡ç›®å¦‚ä¸‹ï¼š", "--- æ•°æ®å¼€å§‹ ---"
    ]
    for i, complaint in enumerate(complaints_data_list):
        details = [];
        for key, value in complaint.items():
            if value is not None: 
                if key == TIME_COLUMN_NAME and isinstance(value, datetime.date): details.append(f"{key}: {value.strftime('%m-%d')}")
                else: details.append(f"{key}: {str(value)}")
        user_prompt_lines.append(f"æŠ•è¯‰ {i+1}: {' | '.join(details)}")
    user_prompt_lines.append("--- æ•°æ®ç»“æŸ ---\nè¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºä¸­çš„è¦æ±‚ï¼Œç”Ÿæˆä½ çš„ä¸“ä¸šåˆ†ææŠ¥å‘Šã€‚")
    full_user_prompt = "\n".join(user_prompt_lines)
    payload = {"model": ollama_model_name, "messages": [{"role": "system", "content": system_prompt_content}, {"role": "user", "content": full_user_prompt}], "stream": False}
    api_endpoint = f"{base_url.rstrip('/')}/api/chat"
    analysis_report = f"æœªèƒ½è¿æ¥åˆ°OllamaæœåŠ¡æˆ–APIè°ƒç”¨å¤±è´¥: {api_endpoint}"
    try:
        headers = {"Content-Type": "application/json"}
        response = requests.post(api_endpoint, data=json.dumps(payload), headers=headers, timeout=300)
        response.raise_for_status()
        response_data = response.json()
        if response_data and 'message' in response_data and 'content' in response_data['message']:
            analysis_report = response_data['message']['content'].strip()
        else: print(f"Ollamaå“åº”ç»“æ„å¼‚å¸¸: {response_data}")
    except requests.exceptions.Timeout: analysis_report = f"LLMåˆ†æè¯·æ±‚è¶…æ—¶(URL:{api_endpoint})"; print(analysis_report)
    except requests.exceptions.HTTPError as e: analysis_report = f"Ollama API HTTPé”™è¯¯(çŠ¶æ€ç  {e.response.status_code}): {e.response.text[:200]}"; print(analysis_report + e.response.text)
    except requests.exceptions.RequestException as e: analysis_report = f"æ— æ³•è¿æ¥åˆ°Ollama: {e}"; print(analysis_report)
    except Exception as e: analysis_report = f"å¤„ç†LLMäº¤äº’æ—¶æœªçŸ¥é”™è¯¯: {e}"; print(analysis_report)
    return analysis_report


# --- 6. Gradio å›è°ƒå‡½æ•°å®šä¹‰ ---

def update_volume_plot_gradio_monthly(): # ç§»é™¤äº†æ—¥æœŸå‚æ•°
    df = fetch_complaint_volume_data_from_db() # ç›´æ¥è°ƒç”¨ï¼Œä¸ä¼ æ—¥æœŸ
    if df.empty or 'æœˆä»½' not in df.columns or 'æ¯æœˆæŠ•è¯‰é‡' not in df.columns:
        return gr.LinePlot(value=None, title="2023-2024å¹´åº¦æœˆåº¦æŠ•è¯‰æ•°æ®é‡è¶‹åŠ¿ - æ— æœ‰æ•ˆæ•°æ®", x_title="æœˆä»½", y_title="æ¯æœˆæŠ•è¯‰é‡")
    return gr.LinePlot(
        df, x='æœˆä»½', y='æ¯æœˆæŠ•è¯‰é‡',
        title="2023-2024å¹´åº¦æœˆåº¦æŠ•è¯‰æ•°æ®é‡è¶‹åŠ¿", # æ›´æ–°æ ‡é¢˜
        x_title="æœˆä»½", y_title="æ¯æœˆæŠ•è¯‰é‡",
        shape="circle", tooltip=['æœˆä»½', 'æ¯æœˆæŠ•è¯‰é‡'],
        height=350, width="auto"
    )

# ç›´æ–¹å›¾å‡½æ•°ä¿æŒä¸å˜ï¼Œå®ƒåŸæœ¬å°±ä¸ä¾èµ–æ—¥æœŸé€‰æ‹©å™¨
def update_type_histogram_gradio():
    df = fetch_complaint_types_distribution_from_db() # è¿™ä¸ªå‡½æ•°å¯èƒ½ä¹Ÿéœ€è¦è°ƒæ•´ä»¥åæ˜ ç‰¹å®šæ—¶æœŸæˆ–å…¨éƒ¨æ•°æ®
    # å¦‚æœå¸Œæœ›ç›´æ–¹å›¾ä¹Ÿä»…æ˜¾ç¤º2023-2024çš„æ•°æ®ï¼Œfetch_complaint_types_distribution_from_db ä¹Ÿéœ€è¦ç±»ä¼¼åœ°è¿‡æ»¤
    if df.empty or 'æŠ•è¯‰ç±»å‹' not in df.columns or 'æ•°é‡' not in df.columns:
        return gr.BarPlot(value=None, title="å„ç±»æŠ•è¯‰æ•°é‡åˆ†å¸ƒ - æ— æœ‰æ•ˆæ•°æ®", x_title="æŠ•è¯‰ç±»å‹", y_title="æ•°é‡")
    return gr.BarPlot(
        df, x='æŠ•è¯‰ç±»å‹', y='æ•°é‡',
        title="å„ç±»æŠ•è¯‰æ•°é‡åˆ†å¸ƒ (ç›´æ–¹å›¾)", # å¯è€ƒè™‘æ·»åŠ å¹´ä»½èŒƒå›´åˆ°æ ‡é¢˜
        x_title="æŠ•è¯‰ç±»å‹", y_title="æ•°é‡",
        vertical_x_text=True, height=350,
        color='æŠ•è¯‰ç±»å‹', width="auto"
    )

def update_type_line_plot_gradio_monthly(): # ç§»é™¤äº†æ—¥æœŸå‚æ•°
    df = fetch_complaint_types_trend_from_db() # ç›´æ¥è°ƒç”¨ï¼Œä¸ä¼ æ—¥æœŸ
    if df.empty or 'æœˆä»½' not in df.columns or 'æŠ•è¯‰ç±»å‹' not in df.columns or 'æ•°é‡' not in df.columns:
        return gr.LinePlot(value=None, title="2023-2024å¹´åº¦å„ç±»æŠ•è¯‰æœˆåº¦æ•°é‡è¶‹åŠ¿ - æ— æœ‰æ•ˆæ•°æ®", x_title="æœˆä»½", y_title="æ•°é‡")
    return gr.LinePlot(
        df, x='æœˆä»½', y='æ•°é‡', color='æŠ•è¯‰ç±»å‹',
        title="2023-2024å¹´åº¦å„ç±»æŠ•è¯‰æœˆåº¦æ•°é‡è¶‹åŠ¿", # æ›´æ–°æ ‡é¢˜
        x_title="æœˆä»½", y_title="æŠ•è¯‰æ•°é‡",
        shape="circle", tooltip=['æœˆä»½', 'æŠ•è¯‰ç±»å‹', 'æ•°é‡'],
        height=350, legend='full', width="auto"
    )

def handle_text2sql_query_gradio(user_question: str, chat_history_proc: list, chat_history_resp: list):
    if not user_question.strip(): return chat_history_proc, chat_history_resp, ""
    if db_answer_instance_for_text2sql is None: 
        err_msg = "é”™è¯¯ï¼šDBSQLAnsweræœªåˆå§‹åŒ–ï¼ŒText2SQLåŠŸèƒ½ä¸å¯ç”¨ã€‚"
        chat_history_proc.append([user_question, err_msg]); chat_history_resp.append([user_question, err_msg])
        return chat_history_proc, chat_history_resp, ""
    process_info, response_info = db_answer_instance_for_text2sql.step_run(user_question)
    response_info = process_info.split("</think>\n\n")[-1]
    chat_history_proc.append([user_question, f"```sql\n{process_info}\n```" if process_info else "-- æ— SQLç”Ÿæˆ --"])
    chat_history_resp.append([user_question, response_info])
    return chat_history_proc, chat_history_resp, ""

def clear_text2sql_chat_gradio(): return [], [], ""

def generate_and_display_alerts_gradio():
    status_update = f"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å¼€å§‹ç”Ÿæˆé¢„è­¦...\n"
    processed_report = "é¢„è­¦æŠ¥å‘Šåˆå§‹åŒ–å†…å®¹..." # Default content
    try:
        # Using the latest 30 records for alert analysis as per your original code
        complaint_data = get_overall_n_newest_complaints(n_records=50)
        analysis_description = "æœ€è¿‘50æ¡æŠ•è¯‰ï¼ˆç”¨äºé¢„è­¦ï¼‰"
        status_update += f"è·å–åˆ° {len(complaint_data)} æ¡æ•°æ®ç”¨äºåˆ†æ ({analysis_description}).\n"
        
        if not complaint_data:
            raw_report = "æ•°æ®ä¸è¶³æˆ–è·å–å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆé¢„è­¦ã€‚"
            status_update += "æ•°æ®ä¸è¶³æˆ–è·å–å¤±è´¥.\n"
        else:
            # This function is assumed to be defined elsewhere in your script
            raw_report = analyze_complaint_trends_with_ollama_via_requests(
                complaint_data, OLLAMA_MODEL_FOR_TREND_ANALYSIS, OLLAMA_BASE_URL_FOR_TRENDS, analysis_description
            )
        
        # Process the report content to remove potential prefixes
        if "</think>" in raw_report:
            processed_report = raw_report.split("</think>")[-1].strip()
        else:
            processed_report = raw_report.strip()
            
        status_update += f"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] é¢„è­¦æŠ¥å‘Šç”Ÿæˆå®Œæ¯•.\n"
        return processed_report, status_update
    except Exception as e:
        error_message = f"ç”Ÿæˆé¢„è­¦å¤±è´¥: {e}"
        status_update += f"é”™è¯¯: {error_message}\n"; print(f"ç”Ÿæˆé¢„è­¦å¼‚å¸¸: {e}")
        # Ensure error messages are also processed if they could contain the prefix
        if "</think>" in error_message:
            processed_report = error_message.split("</think>")[-1].strip()
        else:
            processed_report = error_message.strip()
        return processed_report, status_update

# --- 7. Gradio ç•Œé¢æ„å»º ---
with gr.Blocks(title="12315æ¶ˆè´¹æŠ•è¯‰æ™ºèƒ½åˆ†æå¹³å°", theme=gr.themes.Default()) as demo:
    gr.Markdown("<h1><center>12315æ¶ˆè´¹æŠ•è¯‰æ™ºèƒ½åˆ†æä¸æŸ¥è¯¢å¹³å°</center></h1>")
    with gr.Tabs():
        with gr.TabItem("æ•°æ®æ´å¯Ÿä¸é¢„è­¦", id="tab_visualization"):
            gr.Markdown("## æ¶ˆè´¹æŠ•è¯‰æ´å¯Ÿçœ‹æ¿")
            with gr.Accordion("ğŸš¨ æ¯æ—¥æ¶ˆè´¹è¶‹åŠ¿é¢„è­¦", open=True): # é¢„è­¦éƒ¨åˆ†ä»åŸºäºæœ€æ–°Næ¡ï¼Œä¸æœˆåº¦å›¾è¡¨ç‹¬ç«‹
                with gr.Row():
                    alert_refresh_button = gr.Button("è·å–/åˆ·æ–°æœ€æ–°é¢„è­¦æŠ¥å‘Š", variant="primary", scale=1)
                    alert_status_textbox = gr.Textbox(label="é¢„è­¦ç”ŸæˆçŠ¶æ€", interactive=False, scale=3, lines=2, max_lines=5)
                daily_alert_display = gr.Markdown(label="é¢„è­¦æŠ¥å‘Šå†…å®¹", value="ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®ä»¥ç”Ÿæˆé¢„è­¦æŠ¥å‘Šã€‚")
            
            gr.Markdown("---"); gr.Markdown("### æŠ•è¯‰æ•°æ®ç»Ÿè®¡å›¾è¡¨ (2023-2024å¹´åº¦, æœˆåº¦)") # æ›´æ–°æ ‡é¢˜
            
            plot_refresh_button = gr.Button("åˆ·æ–°æ‰€æœ‰å›¾è¡¨æ•°æ®", variant="secondary")
            
            with gr.Row():
                volume_plot_output = gr.LinePlot(show_label=False)
            with gr.Row():
                type_histogram_output = gr.BarPlot(show_label=False, scale=1)
                type_line_plot_output = gr.LinePlot(show_label=False, scale=1)
            
            alert_refresh_button.click(
                fn=generate_and_display_alerts_gradio, 
                inputs=[], 
                outputs=[daily_alert_display, alert_status_textbox]
            )
            
            # ç»˜å›¾æŒ‰é’®çš„äº‹ä»¶ï¼Œä¸å†éœ€è¦æ—¥æœŸè¾“å…¥
            plot_actions = plot_refresh_button.click(
                fn=update_volume_plot_gradio_monthly, 
                inputs=None, # ç§»é™¤äº† date_picker_inputs
                outputs=[volume_plot_output]
            )
            plot_actions.then(
                fn=update_type_histogram_gradio, 
                inputs=None, 
                outputs=[type_histogram_output]
            )
            plot_actions.then(
                fn=update_type_line_plot_gradio_monthly, 
                inputs=None, # ç§»é™¤äº† date_picker_inputs
                outputs=[type_line_plot_output]
            )
            
            initial_outputs = [
                volume_plot_output, 
                type_histogram_output, 
                type_line_plot_output, 
                daily_alert_display, 
                alert_status_textbox
            ]
            
            def load_all_data_on_start():
                vol_plot_val = update_volume_plot_gradio_monthly()
                hist_plot_val = update_type_histogram_gradio() 
                line_plot_val = update_type_line_plot_gradio_monthly()
                
                alert_report_val, alert_status_val = generate_and_display_alerts_gradio()
                
                return vol_plot_val, hist_plot_val, line_plot_val, alert_report_val, alert_status_val
            
            demo.load(
                load_all_data_on_start, 
                inputs=None, 
                outputs=initial_outputs
            )

        with gr.TabItem("Text-to-SQLæŸ¥è¯¢", id="tab_text2sql"): # Text2SQL éƒ¨åˆ†ä¿æŒä¸å˜
            gr.Markdown("## è‡ªç„¶è¯­è¨€æ•°æ®åº“æŸ¥è¯¢")
            with gr.Row(): 
                with gr.Column(scale=1): 
                    gr.Markdown("### SQL / å¤„ç†è¿‡ç¨‹"); chatbot_process_p2 = gr.Chatbot(label="å¤„ç†è¿‡ç¨‹", height=350, show_copy_button=True, bubble_full_width=False)
                    gr.Markdown("### è‡ªç„¶è¯­è¨€å›å¤"); chatbot_response_p2 = gr.Chatbot(label="æœºå™¨äººå›å¤", height=350, show_copy_button=True, bubble_full_width=False)
                    user_question_p2 = gr.Textbox(label="è¯·è¾“å…¥ä½ çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢:", placeholder="ä¾‹å¦‚ï¼šä¸Šå‘¨äº§å“è´¨é‡ç›¸å…³çš„æŠ•è¯‰æœ‰å¤šå°‘ï¼Ÿ", lines=3)
                    with gr.Row(): send_button_p2 = gr.Button("å‘é€æŸ¥è¯¢", variant="primary", scale=3); clear_button_p2 = gr.Button("æ¸…ç©ºå¯¹è¯", scale=1)
            chat_history_proc_state_p2 = gr.State([]); chat_history_resp_state_p2 = gr.State([])
            send_button_p2.click(handle_text2sql_query_gradio, [user_question_p2, chat_history_proc_state_p2, chat_history_resp_state_p2], [chatbot_process_p2, chatbot_response_p2, user_question_p2])
            user_question_p2.submit(handle_text2sql_query_gradio, [user_question_p2, chat_history_proc_state_p2, chat_history_resp_state_p2], [chatbot_process_p2, chatbot_response_p2, user_question_p2])
            clear_button_p2.click(clear_text2sql_chat_gradio, [], [chatbot_process_p2, chatbot_response_p2, user_question_p2])
# --- 8. ä¸»æ‰§è¡Œå— ---
if __name__ == "__main__":
    print("ç¨‹åºå¯åŠ¨...")
    print("è¯·ç¡®ä¿æ‰€æœ‰æ•°æ®åº“å’ŒOllamaé…ç½®å·²æ­£ç¡®è®¾ç½®åœ¨è„šæœ¬é¡¶éƒ¨ã€‚")
    if OLLAMA_BASE_URL_FOR_TRENDS == "YOUR_OLLAMA_BASE_URL_HERE":
        print("è­¦å‘Š!!! OllamaåŸºç¡€URL (OLLAMA_BASE_URL_FOR_TRENDS) ä»ä¸ºå ä½ç¬¦ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…URLï¼")
    print(f"Ollamaè¶‹åŠ¿åˆ†æå°†ä½¿ç”¨æ¨¡å‹: {OLLAMA_MODEL_FOR_TREND_ANALYSIS} @ {OLLAMA_BASE_URL_FOR_TRENDS}")
    
    print(f"æ­£åœ¨å¯åŠ¨Gradioåº”ç”¨ï¼Œè¯·è®¿é—® http://127.0.0.1:12258 (æˆ–ä½ æŒ‡å®šçš„server_port)")
    demo.launch(server_port=12258, share=False)
    